Chapter 4 NumPyで機械学習を実装する
----------------------------------------
4.1 配列の正規化（normalize）、標準化をする方法
----------------------------------------
4.1.2 z-score normalization（標準化）
----------------------------------------
In [1]:import numpy as np

In [2]:def zscore(x, axis = None):
...:    xmean = x.mean(axis=axis, keepdims=True)
...:    xstd = np.std(x, axis=axis, keepdims=True)
...:    zscore = (x-xmean)/xstd
...:    return zscore

------------------
In [2]:利用しやすいテキスト
------------------
def zscore(x, axis = None):
    xmean = x.mean(axis=axis, keepdims=True)
    xstd = np.std(x, axis=axis, keepdims=True)
    zscore = (x-xmean)/xstd
    return zscore
------------------


In [3]:a = np.random.randint(10, size=(2, 5))

In [4]:a # 乱数を10個生成する

In [5]:zscore(a)

In [6]:zscore(a, axis=1)

In [7]:b = zscore(a, axis=1)

In [8]:b.sum(axis=1)

In [9]:b.std(axis=1)


4.1.3 min-max normalization
----------------------------------------
In [10]:import numpy as np

In [11]:def min_max(x, axis=None):
...:    min = x.min(axis=axis, keepdims=True)
...:    max = x.max(axis=axis, keepdims=True)
...:    result = (x-min)/(max-min)
...:    return result

------------------
In [11]:利用しやすいテキスト
------------------
def min_max(x, axis=None):
    min = x.min(axis=axis, keepdims=True)
    max = x.max(axis=axis, keepdims=True)
    result = (x-min)/(max-min)
    return result
------------------

In [12]:b = np.random.randint(10, size=(2, 5))

In [13]:b

In [14]:c = min_max(b)

In [15]:c

In [16]:d = min_max(b, axis=1)

In [17]:d

4.1.4 ベクトルなどの正規化
----------------------------------------
In [18]:import numpy as np

In [19]:def normalize(v, axis=-1, order=2):
...:    l2 = np.linalg.norm(v, ord=order, axis=axis, keepdims=True)
...:    l2[l2==0] = 1
...:    return v/l2

------------------
In [19]:利用しやすいテキスト
------------------
def normalize(v, axis=-1, order=2):
    l2 = np.linalg.norm(v, ord=order, axis=axis, keepdims=True)
    l2[l2==0] = 1
    return v/l2
------------------

In [20]:a = np.array([1, 2, 3, 2, 1])

In [21]:b = normalize(a)

In [22]:b

In [23]:(b*b).sum()

In [24]:c = np.random.randint(10, size=(3,4))

In [25]:c

In [26]:d = normalize(c, axis=None) #　すべての要素において正規化する

In [27]:d

In [28]:(d*d).sum()

In [29]:e = normalize(c, axis=1)

In [30]:e

In [31]:f = np.random.randint(10, size=(2, 3, 4))

In [32]:normalize(f, axis=(1, 2))

4.2 線形回帰をNumPyで実装する
----------------------------------------
4.2.4 NumPyで実装する
----------------------------------------
In [1]:import numpy as np

In [2]:X = np.random.rand(20)*8-4 # -4～4の範囲での一様乱数

In [3]:X

In [4]:y = np.sin(X) + np.random.randn(20)*0.2 # サインカーブの値にノイズを加える

In [5]:y

In [6]:import matplotlib.pyplot as plt

In [7]:XX = np.linspace(-4, 4, 100) # -4～4の間を100等分した数列を生成する

In [8]:plt.xlabel('X')

In [9]:plt.ylabel('y')

In [10]:plt.title('training data')

In [11]:plt.grid()

In [12]:plt.scatter(X, y, marker='x', c='red') # markerでポイントするもののshape（形状）を指定。cで色を指定。これが散布図になる

In [13]:plt.plot(XX, np.sin(XX)) # サインカーブをプロットする

In [14]:plt.show()

In [15]:A = np.empty((6,6)) # 行列Aの受け皿を作る

In [16]:for i in range(6):
...:    for j in range(6):
...:        A[i][j] = np.sum(X**(i+j))

------------------
In [16]:利用しやすいテキスト
------------------
for i in range(6):
    for j in range(6):
        A[i][j] = np.sum(X**(i+j))
------------------

In [17]:A

In [18]:b = np.empty(6)

In [19]:for i in range(6):
...:    b[i] = np.sum(X**i*y)

------------------
In [19]:利用しやすいテキスト
------------------
for i in range(6):
    b[i] = np.sum(X**i*y)
------------------

In [20]:b

In [21]:omega = np.dot(np.linalg.inv(A), b.reshape(-1, 1)) # linalg.inv()で逆行列を求める。np.dotで内積を求めている

In [22]:omega.shape

In [23]:f = np.poly1d(omega.flatten()[::-1]) # ωを係数とした多項式を作る

In [24]:XX = np.linspace(-4, 4, 100)

In [25]:plt.xlabel('X')

In [26]:plt.ylabel('y')

In [27]:plt.title('trained data')

In [28]:plt.grid()

In [29]:plt.scatter(X, y, marker='x', c='red')

In [30]:plt.plot(XX, f(XX), color='green')

In [31]:plt.plot(XX, np.sin(XX), color='blue')

In [32]:plt.show()

4.2.5 関数で多項式フィッティングを行う
----------------------------------------

In [33]:omega_2 = np.polyfit(X, y, 5)

In [34]:omega_2

In [35]:f_2 = np.poly1d(omega_2)

In [36]:f = np.poly1d(omega.flatten()[::-1])

In [37]:XX = np.linspace(-4, 4, 100)

In [38]:plt.xlabel('X')

In [39]:plt.ylabel('y')

In [40]:plt.title('using polyfitfunction')

In [41]:plt.grid()

In [42]:plt.scatter(X, y, marker='x', c='red')

In [43]:plt.plot(XX, f(XX), color='green')

In [44]:plt.plot(XX, np.sin(XX), color='blue')

In [45]:plt.show()

4.5 NumPyでニューラルネットワークを実装する：実装編
----------------------------------------
4.5.2 データセットの用意
----------------------------------------
［ターミナル］
$ wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data

［ターミナル］
$ pip install pandas

In [1]:import numpy as np
...:import pandas as pd
...:import matplotlib.pyplot as plt
...:from mpl_toolkits.mplot3d import Axes3D
...:df = pd.read_csv('iris.data', header=None) # 先程ダウンロードした'iris.data'を読み込む
...:print(df) # これでデータの中身を表示できる
...:y = df.iloc[0:100,4].values # データの中身を見ると最初の100個分のデータがIris setonaとIris virginicaのものとなっているのでそのラベルデータだけを抜き出す
...:y = np.where(y=='Iris-setona', -1, 1) # ラベルがIris setonaであれば-1、Iris virginicaであれば1として数値を変換する
...:X = df.iloc[0:100,[0, 1, 2, 3]].values # 1～4番目のデータがここの学習に使うものなのでそれを抜き取る

------------------
In [1]:利用しやすいテキスト
------------------
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
df = pd.read_csv('iris.data', header=None) # 先程ダウンロードした'iris.data'を読み込む
print(df) # これでデータの中身を表示できる
y = df.iloc[0:100,4].values # データの中身を見ると最初の100個分のデータがIris setonaとIris virginicaのものとなっているのでそのラベルデータだけを抜き出す
y = np.where(y=='Iris-setona', -1, 1) # ラベルがIris setonaであれば-1、Iris virginicaであれば1として数値を変換する
X = df.iloc[0:100,[0, 1, 2, 3]].values # 1～4番目のデータがここの学習に使うものなのでそれを抜き取る
------------------

4.5.3 訓練用データとテスト用データに分ける
----------------------------------------
In [2]:X_train = np.empty((80, 4)) # データを入れるための空の配列を作る
...:X_test = np.empty((20, 4))
...:y_train = np.empty(80)
...:y_test = np.empty(20)
...:X_train[:40],X_train[40:] = X[:40],X[50:90]
...:X_test[:10],X_test[10:] = X[40:50],X[90:100]
...:y_train[:40],y_train[40:] = y[:40],y[50:90]
...:y_test[:10],y_test[10:] = y[40:50],y[90:100]

------------------
In [2]:利用しやすいテキスト
------------------
X_train = np.empty((80, 4)) # データを入れるための空の配列を作る
X_test = np.empty((20, 4))
y_train = np.empty(80)
y_test = np.empty(20)
X_train[:40],X_train[40:] = X[:40],X[50:90]
X_test[:10],X_test[10:] = X[40:50],X[90:100]
y_train[:40],y_train[40:] = y[:40],y[50:90]
y_test[:10],y_test[10:] = y[40:50],y[90:100]
------------------

In [3]:plt.title('Sepal') # がく片
...:plt.xlabel('length[cm]')
...:plt.ylabel('width[cm]')
...:plt.scatter(X_train[:40, 0], X_train[:40, 1],marker='x', color='blue', label='Iris setosa')
...:plt.scatter(X_train[40:, 0], X_train[40:, 1],marker='o', color='red', label='Iris virginica')
...:plt.legend()
...:plt.show()
...:
...:# 次は花びら
...:plt.title('Petal') # 花びら
...:plt.xlabel('length[cm]')
...:plt.ylabel('width[cm]')
...:plt.scatter(X_train[:40,2], X_train[:40, 3],marker='x', color='blue', label='Iris setosa')
...:plt.scatter(X_train[40:, 2], X_train[40:, 3],marker='o', color='red', label='Iris virginica')
...:plt.legend()
...:plt.show()

------------------
In [3]:利用しやすいテキスト
------------------
plt.title('Sepal') # がく片
plt.xlabel('length[cm]')
plt.ylabel('width[cm]')
plt.scatter(X_train[:40, 0], X_train[:40, 1],marker='x', color='blue', label='Iris setosa')
plt.scatter(X_train[40:, 0], X_train[40:, 1],marker='o', color='red', label='Iris virginica')
plt.legend()
plt.show()

# 次は花びら
plt.title('Petal') # 花びら
plt.xlabel('length[cm]')
plt.ylabel('width[cm]')
plt.scatter(X_train[:40,2], X_train[:40, 3],marker='x', color='blue', label='Iris setosa')
plt.scatter(X_train[40:, 2], X_train[40:, 3],marker='o', color='red', label='Iris virginica')
plt.legend()
plt.show()
------------------

4.5.4　ニューラルネットワークの構築
----------------------------------------
In [4]:def sigmoid(x):
...:    return 1/(1+np.exp(-x))
...:
...:def activation(X, w, b):
...:    return sigmoid(np.dot(X, w)+b)
...:
...:def loss(X, y, w, b):
...:    dif = y - activation(X, w, b)
...:    return np.sum(dif**2/(2*len(y)),keepdims=True)
...:
...:def accuracy(X, y, w, b):
...:    pre = predict(X, w, b)
...:    return np.sum(np.where(pre==y, 1, 0))/len(y)
...:
...:def predict(X, w, b):
...:    result = np.where(activation(X, w, b)<0.5, -1.0, 1.0)
...:    return result
...:
...:def update(X, y, w, b, eta): # 解析的に重みの更新を行う。etaは学習率
...:    a = (activation(X, w, b)-y)*activation(X, w, b)*(1-activation(X, w, b))
...:    a = a.reshape(-1, 1)
...:    w -= eta * 1/float(len(y))*np.sum(a*X, axis=0)
...:    b -= eta * 1/float(len(y))*np.sum(a)
...:    return w, b
...:
...:def update_2(X, y, w, b, eta): # w、bの値をそれぞれ少しだけ増加させた時にどれほど値が変動するかを計算することで偏微分を計算する
...:    h = 1e-4
...:    loss_origin = loss(X, y, w, b)
...:    delta_w = np.zeros_like(w)
...:    delta_b = np.zeros_like(b)
...:    for i in range(4):
...:        tmp = w[i]
...:        w[i] += h # パラメータのうちの1つの値だけを少しだけ増加させる
...:        loss_after = loss(X, y, w, b)
...:        delta_w[i] = eta*(loss_after - loss_origin)/h
...:        w[i] = tmp
...:    tmp = b
...:    b += h
...:    loss_after = loss(X, y, w, b)
...:    delta_b = eta*(loss_after - loss_origin)/h
...:    w -= delta_w # 値を更新する
...:    b = tmp - delta_b
...:    return w, b

------------------
In [4]:利用しやすいテキスト
------------------
def sigmoid(x):
    return 1/(1+np.exp(-x))

def activation(X, w, b):
    return sigmoid(np.dot(X, w)+b)

def loss(X, y, w, b):
    dif = y - activation(X, w, b)
    return np.sum(dif**2/(2*len(y)),keepdims=True)

def accuracy(X, y, w, b):
    pre = predict(X, w, b)
    return np.sum(np.where(pre==y, 1, 0))/len(y)

def predict(X, w, b):
    result = np.where(activation(X, w, b)<0.5, -1.0, 1.0)
    return result

def update(X, y, w, b, eta): # 解析的に重みの更新を行う。etaは学習率
    a = (activation(X, w, b)-y)*activation(X, w, b)*(1-activation(X, w, b))
    a = a.reshape(-1, 1)
    w -= eta * 1/float(len(y))*np.sum(a*X, axis=0)
    b -= eta * 1/float(len(y))*np.sum(a)
    return w, b

def update_2(X, y, w, b, eta): # w、bの値をそれぞれ少しだけ増加させた時にどれほど値が変動するかを計算することで偏微分を計算する
    h = 1e-4
    loss_origin = loss(X, y, w, b)
    delta_w = np.zeros_like(w)
    delta_b = np.zeros_like(b)
    for i in range(4):
        tmp = w[i]
        w[i] += h # パラメータのうちの1つの値だけを少しだけ増加させる
        loss_after = loss(X, y, w, b)
        delta_w[i] = eta*(loss_after - loss_origin)/h
        w[i] = tmp
    tmp = b
    b += h
    loss_after = loss(X, y, w, b)
    delta_b = eta*(loss_after - loss_origin)/h
    w -= delta_w # 値を更新する
    b = tmp - delta_b
    return w, b
------------------

In [5]:weights_1 = np.ones(4)/10 # wの初期値は全部0.1
...:bias_1 = np.ones(1)/10 # bも初期値を0.1にする
...:weights_2 = np.ones(4)/10
...:bias_2 = np.ones(1)/10
...:for _ in range(15): # とりあえず15回ほど学習させる
...:    weights_1, bias_1 = update(X_train, y_train, weights_1, bias_1, eta=0.1)
...:    weights_2, bias_2 = update(X_train , y_train, weights_2, bias_2, eta=0.1)
...:    print('acc_1 %f, loss_1 %f, acc_2 %f, loss_2 %f' % ( accuracy(X_test, y_test, weights_1, bias_1), \
...:    loss(X_test, y_test, weights_1, bias_1)\
    ,accuracy(X_test, y_test, weights_2, bias_2), loss(X_test, y_test, weights_2, bias_2)))
...:print('weights_1 = ', weights_1, 'bias_1 = ', bias_1)
...:print('weights_2 = ', weights_2, 'bias_2 = ', bias_2)


------------------
In [5]:利用しやすいテキスト
------------------
weights_1 = np.ones(4)/10 # wの初期値は全部0.1
bias_1 = np.ones(1)/10 # bも初期値を0.1にする
weights_2 = np.ones(4)/10
bias_2 = np.ones(1)/10
for _ in range(15): # とりあえず15回ほど学習させる
    weights_1, bias_1 = update(X_train, y_train, weights_1, bias_1, eta=0.1)
    weights_2, bias_2 = update(X_train , y_train, weights_2, bias_2, eta=0.1)
    print('acc_1 %f, loss_1 %f, acc_2 %f, loss_2 %f' % ( accuracy(X_test, y_test, weights_1, bias_1), \
    loss(X_test, y_test, weights_1, bias_1)\
    ,accuracy(X_test, y_test, weights_2, bias_2), loss(X_test, y_test, weights_2, bias_2)))
print('weights_1 = ', weights_1, 'bias_1 = ', bias_1)
print('weights_2 = ', weights_2, 'bias_2 = ', bias_2)
------------------


4.7 NumPyでニューラルネットワークを実装する：文字認識編
----------------------------------------
4.7.1　NumPyで実装する（MNIST）
----------------------------------------
［ターミナル］
$ wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
$ wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
$ wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
$ wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz

In [1]:import load_mnist as lm
...:dataset = lm.load_mnist()

------------------
In [1]:利用しやすいテキスト
------------------
import load_mnist as lm
dataset = lm.load_mnist()
------------------

In [2]:import matplotlib.pyplot as plt
...:
...:for i in range(20):
...:    plt.subplot(4, 5, i+1)
...:    plt.imshow(dataset['x_train'][i,:].reshape(28, 28))
...:
...:    plt.show()

------------------
In [2]:利用しやすいテキスト
------------------
import matplotlib.pyplot as plt

for i in range(20):
    plt.subplot(4, 5, i+1)
    plt.imshow(dataset['x_train'][i,:].reshape(28, 28))

    plt.show()
------------------

4.7.3　学習を進める
----------------------------------------
In [3]:import numpy as np
...:import neuralnet as nl
...:import load_mnist
...:
...:dataset = load_mnist.load_mnist()
...:X_train = dataset['x_train']
...:t_train = dataset['t_train']
...:X_test = dataset['x_test']
...:t_test = dataset['t_test']
...:
...:weight_list, bias_list = nl.make_params([784, 100, 10])
...:# 何回学習を行うか指定する
...:train_time = 10000
...:# 1回の学習でいくつのデータを学習するかを指定する
...:batch_size = 1000
...:# 精度と損失がどれだけ変動したかを記録する配列を作る
...:total_acc_list = []
...:total_loss_list = []
...:for i in range(train_time):
...:
...:    # 0～59999でランダムな整数をbatch_size分だけ発生させる
...:    ra = np.random.randint(60000, size=batch_size)
...:    # ここでパラメータの更新を行う。etaは学習率でどれぐらいの割合でパラメータを更新するかを決める
...:    # ここでは学習率を2.0で行う
...:    # 実際は試行錯誤してこの値を決めていくことになる
...:    x_batch, t_batch = X_train[ra,:], t_train[ra,:]
...:    weight_list, bias_list = nl.update(x_batch, weight_list, bias_list, t_batch, eta=2.0)
...:    # 100回ごとにどれぐらい学習できているかを確かめる
...:    if (i+1)%100 == 0:
...:        acc_list = []
...:        loss_list = []
...:        for k in range(10000//batch_size):
...:            x_batch, t_batch = X_test[k*batch_size:(k+1)*batch_size, :], t_test[k*batch_size:(k+1)*batch_size, :]
...:            acc_val = nl.accuracy(x_batch, weight_list, bias_list, t_batch)
...:            loss_val = nl.loss(x_batch, weight_list, bias_list, t_batch)
...:            acc_list.append(acc_val)
...:            loss_list.append(loss_val)
...:        # 精度は平均で求める
...:        acc = np.mean(acc_list)
...:        # 損失は合計で求める
...:        loss = np.mean(loss_list)
...:        total_acc_list.append(acc)
...:        total_loss_list.append(loss)
...:        print("Time: %d, Accuracy: %f, Loss: %f"%(i+1, acc, loss))


------------------
In [3]:利用しやすいテキスト
------------------
import numpy as np
import neuralnet as nl
import load_mnist

dataset = load_mnist.load_mnist()
X_train = dataset['x_train']
t_train = dataset['t_train']
X_test = dataset['x_test']
t_test = dataset['t_test']

weight_list, bias_list = nl.make_params([784, 100, 10])
# 何回学習を行うか指定する
train_time = 10000
# 1回の学習でいくつのデータを学習するかを指定する
batch_size = 1000
# 精度と損失がどれだけ変動したかを記録する配列を作る
total_acc_list = []
total_loss_list = []
for i in range(train_time):

    # 0～59999でランダムな整数をbatch_size分だけ発生させる
    ra = np.random.randint(60000, size=batch_size)
    # ここでパラメータの更新を行う。etaは学習率でどれぐらいの割合でパラメータを更新するかを決める
    # ここでは学習率を2.0で行う
    # 実際は試行錯誤してこの値を決めていくことになる
    x_batch, t_batch = X_train[ra,:], t_train[ra,:]
    weight_list, bias_list = nl.update(x_batch, weight_list, bias_list, t_batch, eta=2.0)
    # 100回ごとにどれぐらい学習できているかを確かめる
    if (i+1)%100 == 0:
        acc_list = []
        loss_list = []
        for k in range(10000//batch_size):
            x_batch, t_batch = X_test[k*batch_size:(k+1)*batch_size, :], t_test[k*batch_size:(k+1)*batch_size, :]
            acc_val = nl.accuracy(x_batch, weight_list, bias_list, t_batch)
            loss_val = nl.loss(x_batch, weight_list, bias_list, t_batch)
            acc_list.append(acc_val)
            loss_list.append(loss_val)
        # 精度は平均で求める
        acc = np.mean(acc_list)
        # 損失は合計で求める
        loss = np.mean(loss_list)
        total_acc_list.append(acc)
        total_loss_list.append(loss)
        print("Time: %d, Accuracy: %f, Loss: %f"%(i+1, acc, loss))
------------------

In [4]:import matplotlib.pyplot as plt
...:plt.subplot(211)
...:plt.plot(np.arange(0, train_time, 100),total_acc_list)
...:plt.title('accuracy')
...:plt.subplot(212)
...:plt.plot(np.arange(0, train_time, 100), total_loss_list)
...:plt.title('loss')
...:plt.tight_layout()
...:plt.show()

------------------
In [4]:利用しやすいテキスト
------------------
import matplotlib.pyplot as plt
plt.subplot(211)
plt.plot(np.arange(0, train_time, 100),total_acc_list)
plt.title('accuracy')
plt.subplot(212)
plt.plot(np.arange(0, train_time, 100), total_loss_list)
plt.title('loss')
plt.tight_layout()
plt.show()
------------------

4.8 NumPyで強化学習を実装する
----------------------------------------
4.8.2　インストールからゲームの実行まで
----------------------------------------
［ターミナル］
$ pip install gym

In [1]:import gym
...:env = gym.make("CartPole-v0")

------------------
In [1]:利用しやすいテキスト
------------------
import gym
env = gym.make("CartPole-v0")
------------------

In [2]:observation = env.reset()

In [3]:action = 1 # とりあえず右に押す
...:observation, reward, done, info = env.step(action) # stepを実行すると行動を起こした直後の状態、報酬、ゲームが終了したかどうか、情報の4つの変数が返される

------------------
In [3]:利用しやすいテキスト
------------------
action = 1 # とりあえず右に押す
observation, reward, done, info = env.step(action) # stepを実行すると行動を起こした直後の状態、報酬、ゲームが終了したかどうか、情報の4つの変数が返される
------------------

In [4]:env.render()

In [5]:import numpy as np
...:observation = env.reset()
...:
...:for k in range(100):
...:    env.render()
...:    # 0か1の乱数で実行
...:    observation, reward, done, info=env.step(np.random.randint(1))
...:# 終了する時はenv.close()を実行する必要がある
...:env.close()

------------------
In [5]:利用しやすいテキスト
------------------
import numpy as np
observation = env.reset()

for k in range(100):
    env.render()
    # 0か1の乱数で実行
    observation, reward, done, info=env.step(np.random.randint(1))
# 終了する時はenv.close()を実行する必要がある
env.close()
------------------

4.8.3　Q学習
----------------------------------------
［ターミナル］
$ python cartpole1.py

［ターミナル］
$ python cartpole2.py

［ターミナル］
$ python cartpole3.py

［ターミナル］
$ python cartpole4.py

［ターミナル］
$ python cartpole5.py